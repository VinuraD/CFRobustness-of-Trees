#!/usr/bin/env python3
"""
Simple usage example for the robustness testing framework.
Demonstrates the simplified interface: get_data(fold, raw_data=True/False)
All terminal output is logged to a timestamped log file for analysis.
"""

import sys
import os
import logging
from datetime import datetime
sys.path.append(os.path.join(os.path.dirname(__file__), 'modules'))

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import LabelEncoder

from data_module import DataModule
from perturb import Perturbation

def setup_logging():
    """Setup comprehensive logging to both console and file"""
    # Create timestamp for log file
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_filename = f"robustness_analysis_{timestamp}.log"
    
    # Create logger
    logger = logging.getLogger('RobustnessFramework')
    logger.setLevel(logging.INFO)
    
    # Clear any existing handlers
    logger.handlers.clear()
    
    # Create formatters
    file_formatter = logging.Formatter(
        '%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    console_formatter = logging.Formatter('%(message)s')
    
    # File handler
    file_handler = logging.FileHandler(log_filename, mode='w', encoding='utf-8')
    file_handler.setLevel(logging.INFO)
    file_handler.setFormatter(file_formatter)
    logger.addHandler(file_handler)
    
    # Console handler
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(console_formatter)
    logger.addHandler(console_handler)
    
    return logger, log_filename

def log_print(*args, **kwargs):
    """Enhanced print function that logs to both console and file"""
    # Convert all arguments to strings and join them
    message = ' '.join(str(arg) for arg in args)
    
    # Get the logger
    logger = logging.getLogger('RobustnessFramework')
    
    # Log the message
    logger.info(message)

def main():
    # Setup logging
    logger, log_filename = setup_logging()
    
    log_print("=" * 80)
    log_print("COMPREHENSIVE ROBUSTNESS TESTING FRAMEWORK")
    log_print("=" * 80)
    log_print(f"üìù Logging session to: {log_filename}")
    log_print(f"üïí Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    log_print("=" * 80)
    
    # 1. Load dataset
    log_print("\n1. Loading dataset...")
    dm = DataModule("data/Spambase.csv", n_splits=5, random_state=42)
    perturbation = Perturbation(dm)
    
    # Get metadata
    metadata = perturbation.get_metadata()
    log_print(f"Label column: {metadata['label_column']}")
    log_print(f"Feature types: {len(metadata['feature_types'])} features")
    log_print(f"Feature actions: {set(metadata['feature_actions'].values())}")
    
    # Print fold summary - capture output and log it
    log_print("\nFold Summary:")
    import io
    import contextlib
    
    # Capture the output from dm.print_fold_summary()
    f = io.StringIO()
    with contextlib.redirect_stdout(f):
        dm.print_fold_summary()
    
    # Log each line of the captured output
    summary_output = f.getvalue()
    for line in summary_output.strip().split('\n'):
        if line.strip():  # Only log non-empty lines
            log_print(line)
    
    # 2. Comprehensive perturbation testing across all folds
    log_print("\n2. Comprehensive Perturbation Testing - All 5 Folds")
    log_print("Note: Rows with missing values have been completely removed.")
    log_print("=" * 80)
    
    # Initialize result storage for averaging
    results = {
        'baseline': [],
        'minor_deletion': {i: [] for i in range(0, 21, 2)},
        'major_deletion': {i: [] for i in range(2)},
        'minor_addition': {i: [] for i in range(0, 21, 2)},
        'major_addition': {i: [] for i in range(2)},
        'models': {
            'random_forest_3_50': [],
            'random_forest_5_100': [],
            'random_forest_10_200': [],
            'xgboost_3_50': [],
            'xgboost_6_100': [],
            'lightgbm_4_75': [],
            'adaboost_2_30': []
        }
    }
    
    # Run comprehensive testing on all folds
    for fold in range(5):
        log_print(f"\n--- FOLD {fold} - COMPREHENSIVE PERTURBATION ANALYSIS ---")
        
        # Get raw data
        train_raw, test_raw = perturbation.get_data(fold=fold, raw_data=True)
        log_print(f"Raw data - Train: {train_raw.shape}, Test: {test_raw.shape}")
        
        # Verify no missing values in raw data
        train_missing = train_raw.isnull().sum().sum()
        test_missing = test_raw.isnull().sum().sum()
        log_print(f"Missing values - Train: {train_missing}, Test: {test_missing}")
        
        # Get preprocessed data
        train_processed, test_processed = perturbation.get_data(fold=fold, raw_data=False)
        log_print(f"Processed data - Train: {train_processed.shape}, Test: {test_processed.shape}")
        
        # Verify no missing values in processed data
        train_proc_missing = train_processed.isnull().sum().sum()
        test_proc_missing = test_processed.isnull().sum().sum()
        log_print(f"Missing values (processed) - Train: {train_proc_missing}, Test: {test_proc_missing}")
        
        # Setup for all experiments
        label_col = metadata['label_column']
        X_train = train_processed.drop(columns=[label_col])
        y_train = train_processed[label_col]
        X_test = test_processed.drop(columns=[label_col])
        y_test = test_processed[label_col]
        
        # Handle categorical labels
        if y_train.dtype == 'object':
            le = LabelEncoder()
            y_train = le.fit_transform(y_train)
            y_test = le.transform(y_test)
        
        # Train baseline model
        model = RandomForestClassifier(n_estimators=100, random_state=42)
        model.fit(X_train, y_train)
        baseline_acc = accuracy_score(y_test, model.predict(X_test))
        results['baseline'].append(baseline_acc)
        log_print(f"\nBaseline model - Training size: {len(X_train)} samples")
        log_print(f"Baseline accuracy: {baseline_acc:.4f}")
    
        # 3. ALL DATA PERTURBATIONS
        log_print(f"\n{'='*80}")
        log_print("ALL DATA PERTURBATIONS")
        log_print(f"{'='*80}")
        
        # MINOR DELETION (21 bins: 0-20%)
        log_print(f"\n--- MINOR DELETION (Remove 0%-20%) ---")
        for bin_num in range(0, 21, 2):  # Test every 2nd bin to reduce output
            train_pert = perturbation.perturb_data(train_processed, 'minor_deletion', bin_num)
            X_pert = train_pert.drop(columns=[label_col])
            y_pert = train_pert[label_col]
            if y_pert.dtype == 'object':
                y_pert = le.transform(y_pert)
            
            model_pert = RandomForestClassifier(n_estimators=100, random_state=42)
            model_pert.fit(X_pert, y_pert)
            pert_acc = accuracy_score(y_test, model_pert.predict(X_test))
            results['minor_deletion'][bin_num].append(pert_acc)
            log_print(f"  Bin {bin_num:2d}: Remove {bin_num:2d}% - Training size: {len(X_pert):3d} samples - Accuracy: {pert_acc:.4f} (Œî: {pert_acc - baseline_acc:+.4f})")
        
        # MAJOR DELETION (2 bins: 0%, 50%)
        log_print(f"\n--- MAJOR DELETION (Remove 0%, 50%) ---")
        for bin_num in range(2):
            train_pert = perturbation.perturb_data(train_processed, 'major_deletion', bin_num)
            X_pert = train_pert.drop(columns=[label_col])
            y_pert = train_pert[label_col]
            if y_pert.dtype == 'object':
                y_pert = le.transform(y_pert)
            
            model_pert = RandomForestClassifier(n_estimators=100, random_state=42)
            model_pert.fit(X_pert, y_pert)
            pert_acc = accuracy_score(y_test, model_pert.predict(X_test))
            results['major_deletion'][bin_num].append(pert_acc)
            remove_pct = 0 if bin_num == 0 else 50
            log_print(f"  Bin {bin_num}: Remove {remove_pct:2d}% - Training size: {len(X_pert):3d} samples - Accuracy: {pert_acc:.4f} (Œî: {pert_acc - baseline_acc:+.4f})")
        
        # MINOR ADDITION (21 bins: 80%-100%)
        log_print(f"\n--- MINOR ADDITION (Use 80%-100%) ---")
        for bin_num in range(0, 21, 2):  # Test every 2nd bin to reduce output
            train_pert = perturbation.perturb_data(train_processed, 'minor_addition', bin_num)
            X_pert = train_pert.drop(columns=[label_col])
            y_pert = train_pert[label_col]
            if y_pert.dtype == 'object':
                y_pert = le.transform(y_pert)
            
            model_pert = RandomForestClassifier(n_estimators=100, random_state=42)
            model_pert.fit(X_pert, y_pert)
            pert_acc = accuracy_score(y_test, model_pert.predict(X_test))
            results['minor_addition'][bin_num].append(pert_acc)
            use_pct = 80 + bin_num
            log_print(f"  Bin {bin_num:2d}: Use {use_pct:3d}% - Training size: {len(X_pert):3d} samples - Accuracy: {pert_acc:.4f} (Œî: {pert_acc - baseline_acc:+.4f})")
        
        # MAJOR ADDITION (2 bins: 50%, 100%)
        log_print(f"\n--- MAJOR ADDITION (Use 50%, 100%) ---")
        for bin_num in range(2):
            train_pert = perturbation.perturb_data(train_processed, 'major_addition', bin_num)
            X_pert = train_pert.drop(columns=[label_col])
            y_pert = train_pert[label_col]
            if y_pert.dtype == 'object':
                y_pert = le.transform(y_pert)
            
            model_pert = RandomForestClassifier(n_estimators=100, random_state=42)
            model_pert.fit(X_pert, y_pert)
            pert_acc = accuracy_score(y_test, model_pert.predict(X_test))
            results['major_addition'][bin_num].append(pert_acc)
            use_pct = 50 if bin_num == 0 else 100
            log_print(f"  Bin {bin_num}: Use {use_pct:3d}% - Training size: {len(X_pert):3d} samples - Accuracy: {pert_acc:.4f} (Œî: {pert_acc - baseline_acc:+.4f})")
    
        # 4. MODEL PERTURBATIONS
        log_print(f"\n{'='*80}")
        log_print("MODEL PERTURBATIONS (Using 80% of training data)")
        log_print(f"{'='*80}")
        
        model_configs = [
            ('random_forest', 3, 50, 'random_forest_3_50'),
            ('random_forest', 5, 100, 'random_forest_5_100'),
            ('random_forest', 10, 200, 'random_forest_10_200'),
            ('xgboost', 3, 50, 'xgboost_3_50'),
            ('xgboost', 6, 100, 'xgboost_6_100'),
            ('lightgbm', 4, 75, 'lightgbm_4_75'),
            ('adaboost', 2, 30, 'adaboost_2_30'),
        ]
        
        for model_type, max_depth, n_estimators, result_key in model_configs:
            try:
                # perturb_model uses 80% of the training data internally
                model_training_size = int(len(train_processed) * 0.8)
                model_pert = perturbation.perturb_model(train_processed, model_type, max_depth, n_estimators)
                model_acc = accuracy_score(y_test, model_pert.predict(X_test))
                results['models'][result_key].append(model_acc)
                log_print(f"  {model_type.upper():12s} (depth={max_depth:2d}, n_est={n_estimators:3d}) - Training size: {model_training_size:3d} samples - Accuracy: {model_acc:.4f} (Œî: {model_acc - baseline_acc:+.4f})")
            except ImportError as e:
                log_print(f"  {model_type.upper():12s} - Not available: {e}")
                results['models'][result_key].append(None)  # Mark as unavailable
            except Exception as e:
                log_print(f"  {model_type.upper():12s} - Failed: {e}")
                results['models'][result_key].append(None)  # Mark as failed
    
    # Calculate and display summary statistics
    log_print(f"\n{'='*100}")
    log_print("SUMMARY: AVERAGED RESULTS ACROSS ALL 5 FOLDS")
    log_print(f"{'='*100}")
    
    import numpy as np
    
    # Baseline results
    baseline_mean = np.mean(results['baseline'])
    baseline_std = np.std(results['baseline'])
    log_print(f"\nBASELINE PERFORMANCE:")
    log_print(f"  Mean Accuracy: {baseline_mean:.4f} ¬± {baseline_std:.4f}")
    log_print(f"  Individual Folds: {[f'{acc:.4f}' for acc in results['baseline']]}")
    
    # Data perturbation results
    log_print(f"\nDATA PERTURBATIONS - AVERAGED ACCURACY:")
    log_print(f"{'Type':<20} {'Bin':<4} {'Remove/Use':<10} {'Mean Acc':<10} {'Std':<8} {'Œî vs Baseline':<12} {'All Folds'}")
    log_print("-" * 100)
    
    # Minor deletion
    for bin_num in range(0, 21, 2):
        accs = results['minor_deletion'][bin_num]
        if accs:
            mean_acc = np.mean(accs)
            std_acc = np.std(accs)
            delta = mean_acc - baseline_mean
            log_print(f"{'Minor Deletion':<20} {bin_num:<4} {f'Remove {bin_num}%':<10} {mean_acc:<10.4f} {std_acc:<8.4f} {delta:<+12.4f} {[f'{acc:.4f}' for acc in accs]}")
    
    # Major deletion
    for bin_num in range(2):
        accs = results['major_deletion'][bin_num]
        if accs:
            mean_acc = np.mean(accs)
            std_acc = np.std(accs)
            delta = mean_acc - baseline_mean
            remove_pct = 0 if bin_num == 0 else 50
            log_print(f"{'Major Deletion':<20} {bin_num:<4} {f'Remove {remove_pct}%':<10} {mean_acc:<10.4f} {std_acc:<8.4f} {delta:<+12.4f} {[f'{acc:.4f}' for acc in accs]}")
    
    # Minor addition
    for bin_num in range(0, 21, 2):
        accs = results['minor_addition'][bin_num]
        if accs:
            mean_acc = np.mean(accs)
            std_acc = np.std(accs)
            delta = mean_acc - baseline_mean
            use_pct = 80 + bin_num
            log_print(f"{'Minor Addition':<20} {bin_num:<4} {f'Use {use_pct}%':<10} {mean_acc:<10.4f} {std_acc:<8.4f} {delta:<+12.4f} {[f'{acc:.4f}' for acc in accs]}")
    
    # Major addition
    for bin_num in range(2):
        accs = results['major_addition'][bin_num]
        if accs:
            mean_acc = np.mean(accs)
            std_acc = np.std(accs)
            delta = mean_acc - baseline_mean
            use_pct = 50 if bin_num == 0 else 100
            log_print(f"{'Major Addition':<20} {bin_num:<4} {f'Use {use_pct}%':<10} {mean_acc:<10.4f} {std_acc:<8.4f} {delta:<+12.4f} {[f'{acc:.4f}' for acc in accs]}")
    
    # Model perturbation results
    log_print(f"\nMODEL PERTURBATIONS - AVERAGED ACCURACY:")
    log_print(f"{'Model':<25} {'Parameters':<20} {'Mean Acc':<10} {'Std':<8} {'Œî vs Baseline':<12} {'Available':<10} {'All Folds'}")
    log_print("-" * 110)
    
    model_display = [
        ('random_forest_3_50', 'Random Forest', 'depth=3, n_est=50'),
        ('random_forest_5_100', 'Random Forest', 'depth=5, n_est=100'),
        ('random_forest_10_200', 'Random Forest', 'depth=10, n_est=200'),
        ('xgboost_3_50', 'XGBoost', 'depth=3, n_est=50'),
        ('xgboost_6_100', 'XGBoost', 'depth=6, n_est=100'),
        ('lightgbm_4_75', 'LightGBM', 'depth=4, n_est=75'),
        ('adaboost_2_30', 'AdaBoost', 'depth=2, n_est=30'),
    ]
    
    for result_key, model_name, params in model_display:
        accs = [acc for acc in results['models'][result_key] if acc is not None]
        if accs:
            mean_acc = np.mean(accs)
            std_acc = np.std(accs)
            delta = mean_acc - baseline_mean
            available = f"{len(accs)}/5 folds"
            log_print(f"{model_name:<25} {params:<20} {mean_acc:<10.4f} {std_acc:<8.4f} {delta:<+12.4f} {available:<10} {[f'{acc:.4f}' for acc in accs]}")
        else:
            log_print(f"{model_name:<25} {params:<20} {'N/A':<10} {'N/A':<8} {'N/A':<12} {'0/5 folds':<10} Not Available")
    
    # Summary insights
    log_print(f"\n{'='*100}")
    log_print("KEY INSIGHTS FROM GERMAN CREDIT DATASET:")
    log_print(f"{'='*100}")
    
    # Find best and worst perturbations
    all_data_results = []
    for pert_type in ['minor_deletion', 'major_deletion', 'minor_addition', 'major_addition']:
        for bin_num, accs in results[pert_type].items():
            if accs:
                mean_acc = np.mean(accs)
                all_data_results.append((pert_type, bin_num, mean_acc))
    
    if all_data_results:
        best_data_pert = max(all_data_results, key=lambda x: x[2])
        worst_data_pert = min(all_data_results, key=lambda x: x[2])
        
        log_print(f"‚Ä¢ Baseline Performance: {baseline_mean:.4f} ¬± {baseline_std:.4f}")
        log_print(f"‚Ä¢ Best Data Perturbation: {best_data_pert[0]} bin {best_data_pert[1]} (Accuracy: {best_data_pert[2]:.4f})")
        log_print(f"‚Ä¢ Worst Data Perturbation: {worst_data_pert[0]} bin {worst_data_pert[1]} (Accuracy: {worst_data_pert[2]:.4f})")
        log_print(f"‚Ä¢ Data Perturbation Range: {worst_data_pert[2]:.4f} to {best_data_pert[2]:.4f} (Œî: {best_data_pert[2] - worst_data_pert[2]:.4f})")
    
    # Model performance summary
    valid_model_results = [(key, np.mean([acc for acc in accs if acc is not None])) 
                          for key, accs in results['models'].items() 
                          if any(acc is not None for acc in accs)]
    
    if valid_model_results:
        best_model = max(valid_model_results, key=lambda x: x[1])
        worst_model = min(valid_model_results, key=lambda x: x[1])
        log_print(f"‚Ä¢ Best Model: {best_model[0]} (Accuracy: {best_model[1]:.4f})")
        log_print(f"‚Ä¢ Worst Model: {worst_model[0]} (Accuracy: {worst_model[1]:.4f})")
        log_print(f"‚Ä¢ Model Performance Range: {worst_model[1]:.4f} to {best_model[1]:.4f} (Œî: {best_model[1] - worst_model[1]:.4f})")
    
    log_print(f"‚Ä¢ Total Experiments: {5 * (len([0] + list(range(0, 21, 2)) + [0, 1] + list(range(0, 21, 2)) + [0, 1]) + len([key for key in results['models'].keys()]))} across 5 folds")
    log_print(f"‚Ä¢ Robustness Framework: Systematic evaluation complete!")
    
    # 5. Demonstrate with another dataset
    log_print("\n3. Testing with another dataset (HELOC)...")
    try:
        dm_heloc = DataModule("data/heloc_dataset_v1_withmeta.csv", n_splits=3, random_state=42)
        perturbation_heloc = Perturbation(dm_heloc)
        
        # Get data for fold 0
        train_heloc, test_heloc = perturbation_heloc.get_data(fold=0, raw_data=True)
        log_print(f"HELOC raw data - Train: {train_heloc.shape}, Test: {test_heloc.shape}")
        
        # Get preprocessed data
        train_heloc_proc, test_heloc_proc = perturbation_heloc.get_data(fold=0, raw_data=False)
        log_print(f"HELOC processed data - Train: {train_heloc_proc.shape}, Test: {test_heloc_proc.shape}")
        
        # Get metadata
        metadata_heloc = perturbation_heloc.get_metadata()
        log_print(f"HELOC label column: {metadata_heloc['label_column']}")
        
    except Exception as e:
        log_print(f"Note: HELOC dataset test skipped - {e}")
    
    # Add completion message and timing
    end_time = datetime.now()
    log_print(f"\n{'='*100}")
    log_print("üéØ COMPREHENSIVE ROBUSTNESS TESTING FRAMEWORK DEMO COMPLETED!")
    log_print(f"{'='*100}")
    log_print(f"üïí Completed at: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
    log_print(f"‚è±Ô∏è Total analysis time captured in log file")
    log_print(f"üìù All results saved to log file for further analysis")
    log_print(f"{'='*100}")
    
    log_print("\nüìä DEMO SUMMARY:")
    log_print("‚úÖ Comprehensive testing of ALL perturbation variations on German Credit Dataset")
    log_print("‚úÖ All 5 folds analyzed with complete perturbation coverage")
    log_print("‚úÖ Training sizes displayed for every model training occasion")
    log_print("‚úÖ Accuracy changes (Œî) calculated relative to baseline for each fold")
    log_print("‚úÖ Averaged results across all folds with standard deviations")
    log_print("‚úÖ Both data and model perturbations systematically evaluated")
    log_print("‚úÖ Best/worst perturbations identified with performance ranges")
    log_print("‚úÖ Statistical summary with confidence intervals")
    log_print("\nüîß KEY FEATURES:")
    log_print("‚úÖ Simple interface: get_data(fold, raw_data=True/False)")
    log_print("‚úÖ K-fold CV on entire dataset (ignoring metadata rows)")
    log_print("‚úÖ Rows with missing values completely removed")
    log_print("‚úÖ Label encoding for categorical features (tree-friendly)")
    log_print("‚úÖ MinMax scaling for numeric features")
    log_print("‚úÖ Systematic bin-based data perturbations")
    log_print("‚úÖ Integrated model training with hyperparameter perturbations")
    log_print("‚úÖ Support for multiple model types (RF, XGBoost, LightGBM, AdaBoost)")
    log_print("‚úÖ Comprehensive robustness analysis with statistical averaging")
    log_print("\nüíª USAGE:")
    log_print("  dm = DataModule('dataset.csv')")
    log_print("  perturbation = Perturbation(dm)")
    log_print("  train, test = perturbation.get_data(fold=0, raw_data=True)")
    log_print("  # Data perturbations")
    log_print("  perturbed = perturbation.perturb_data(train, 'minor_deletion', bin_number=10)")
    log_print("  # Model perturbations")
    log_print("  model = perturbation.perturb_model(train, 'random_forest', max_depth=5, n_estimators=100)")
    log_print("\nüìà DATA PERTURBATION COVERAGE:")
    log_print("  üîπ minor_deletion: 21 bins (remove 0%-20%) - Tested every 2nd bin across 5 folds")
    log_print("  üîπ major_deletion: 2 bins (remove 0%, 50%) - All bins tested across 5 folds")
    log_print("  üîπ minor_addition: 21 bins (use 80%-100%) - Tested every 2nd bin across 5 folds")
    log_print("  üîπ major_addition: 2 bins (use 50%, 100%) - All bins tested across 5 folds")
    log_print("\nü§ñ MODEL PERTURBATION COVERAGE:")
    log_print("  üîπ Multiple algorithms with various hyperparameter combinations")
    log_print("  üîπ All models use 80% of training data for consistent comparison")
    log_print("  üîπ Error handling for unavailable models")
    log_print("  üîπ Results averaged across successful folds")
    log_print("\n‚öôÔ∏è PREPROCESSING:")
    log_print("  üîπ Categorical features: Label encoded (C ‚Üí D)")
    log_print("  üîπ Numeric features: MinMax scaled (N, D, B ‚Üí scaled)")
    log_print("  üîπ Perfect for tree-based algorithms!")
    log_print("\nüéØ ROBUSTNESS INSIGHTS:")
    log_print("  üîπ Training size impact clearly visible")
    log_print("  üîπ Performance changes (Œî) relative to baseline with confidence intervals")
    log_print("  üîπ Systematic evaluation across all perturbation types")
    log_print("  üîπ Statistical significance with mean ¬± standard deviation")
    log_print("  üîπ Best/worst performing configurations identified")
    log_print("  üîπ Ready for comprehensive robustness analysis and research!")
    log_print("\nüèÜ FRAMEWORK BENEFITS:")
    log_print("  üîπ Reproducible and systematic robustness evaluation")
    log_print("  üîπ Statistical rigor with cross-fold validation")
    log_print("  üîπ Clear performance comparisons and rankings")
    log_print("  üîπ Publication-ready robustness analysis results")

if __name__ == "__main__":
    main() 