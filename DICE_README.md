# DiCE Counterfactual Generation for Robustness Framework

This module provides comprehensive counterfactual generation using DiCE ML specifically for **Fold 0 (Bin 0)** analysis on the **Spambase dataset**. It generates counterfactuals to flip class labels and calculates detailed quality metrics.

## ðŸŽ¯ Overview

The `dice_counterfactual_generation.py` script:
- Loads the Spambase dataset and focuses on fold 0 
- Trains a Random Forest baseline model
- Generates counterfactuals using DiCE ML random method
- Calculates comprehensive quality metrics:
  - **Validity**: Percentage of counterfactuals that actually flip class predictions
  - **L2 Distance**: Average Euclidean distance between originals and counterfactuals
  - **LOF Score**: Local Outlier Factor measuring how outlier-like counterfactuals are
  - **Sparsity**: Average number of features changed per counterfactual

## ðŸ“¦ Installation

### Step 1: Check Dependencies
```bash
python check_dice_installation.py
```

### Step 2: Install DiCE ML (if needed)
```bash
# Option 1: Direct installation
pip install dice-ml

# Option 2: From requirements file
pip install -r requirements_dice.txt

# Option 3: With specific version
pip install dice-ml>=0.9
```

### Step 3: Verify Installation
```bash
python check_dice_installation.py
```

## ðŸš€ Usage

### Basic Execution
```bash
python dice_counterfactual_generation.py
```

### What the Script Does

1. **ðŸ“Š Data Loading**
   - Loads Spambase.csv dataset
   - Extracts fold 0 (train/test split)
   - Preprocesses data using the robustness framework

2. **ðŸ¤– Model Training**
   - Trains Random Forest classifier (100 trees)
   - Reports baseline performance on train/test sets

3. **ðŸ”„ Counterfactual Generation**
   - Uses DiCE ML with random method
   - Attempts to generate counterfactuals for all test samples
   - Reports success rate and failure handling

4. **ðŸ“ˆ Quality Analysis**
   - **Validity**: Measures actual class flipping effectiveness
   - **Distance**: Calculates similarity to original samples
   - **LOF**: Evaluates how normal/outlier-like CFs are
   - **Sparsity**: Counts average features changed

5. **ðŸ“ Comprehensive Logging**
   - All output logged to timestamped file: `dice_counterfactual_analysis_YYYYMMDD_HHMMSS.log`
   - Real-time console output for monitoring

## ðŸ“Š Output Metrics

### Success Metrics
- **Success Rate**: Percentage of test samples that got valid counterfactuals
- **Generation Rate**: Overall CF generation statistics

### Quality Metrics
- **Validity Score**: 0.0-1.0 (higher = better class flipping)
- **L2 Distance**: Lower values = more similar to originals
- **LOF Score**: Closer to -1.0 = more normal (less outlier-like)  
- **Sparsity**: Lower values = fewer features changed

## ðŸ“ File Structure

```
CFRobustness-of-Trees-main/
â”œâ”€â”€ dice_counterfactual_generation.py    # Main script
â”œâ”€â”€ check_dice_installation.py           # Dependency checker
â”œâ”€â”€ requirements_dice.txt                # DiCE ML requirements
â”œâ”€â”€ DICE_README.md                      # This documentation
â”œâ”€â”€ data/
â”‚   â””â”€â”€ Spambase.csv                    # Target dataset
â””â”€â”€ modules/
    â”œâ”€â”€ data_module.py                  # Data handling
    â””â”€â”€ perturb.py                      # Perturbation framework
```

## ðŸ”§ Technical Details

### DiCE Configuration
- **Method**: Random counterfactual generation
- **Backend**: sklearn
- **CFs per sample**: 2 (uses first successful one)
- **Desired class**: "opposite" (automatic class flipping)

### Data Handling
- **Dataset**: Spambase (spam classification)
- **Fold**: 0 (first fold of 5-fold CV)
- **Preprocessing**: Uses robustness framework's preprocessing pipeline
- **Features**: All features treated as continuous for DiCE

### Model Configuration
- **Algorithm**: Random Forest
- **Trees**: 100
- **Random State**: 42 (reproducible results)

## ðŸ“ˆ Expected Results

### Typical Output
```
ðŸ“ˆ COMPREHENSIVE COUNTERFACTUAL ANALYSIS SUMMARY
================================================================================

ðŸŽ¯ DATASET: Spambase (Fold 0)
  â€¢ Training samples: 2763
  â€¢ Test samples: 921
  â€¢ Features: 57
  â€¢ Baseline test accuracy: 0.9250

ðŸ”§ COUNTERFACTUAL GENERATION:
  â€¢ Method: Random
  â€¢ Success rate: 85.2%
  â€¢ Successful CFs: 785/921

ðŸ“Š QUALITY METRICS:
  â€¢ Validity: 0.9100 (91.0%)
  â€¢ Average L2 distance: 2.4500
  â€¢ Average LOF score: -1.1200
  â€¢ Average sparsity: 8.50 features
```

### Interpretation Guide
- **High Validity (>0.8)**: Good counterfactuals that flip classes
- **Low L2 Distance (<5.0)**: Similar to original samples
- **LOF near -1.0**: Normal counterfactuals (not outliers)
- **Low Sparsity (<20% features)**: Minimal changes needed

## ðŸš¨ Error Handling

The script includes comprehensive error handling for:
- Missing DiCE ML installation
- Dataset loading issues
- Model training failures
- Counterfactual generation failures
- Metric calculation errors

If counterfactual generation fails for specific samples, they are marked as failed and excluded from quality metrics calculation.

## ðŸ”„ Integration with Robustness Framework

This script integrates seamlessly with the existing robustness framework:
- Uses same `DataModule` for consistent data handling
- Uses same preprocessing pipeline
- Focuses on fold 0 to complement perturbation analysis
- Provides logged output compatible with framework standards

## ðŸ“Š Research Applications

This tool is designed for:
- **Robustness Analysis**: Understanding model behavior through counterfactuals
- **Fairness Research**: Analyzing biased decision boundaries
- **Explainability Studies**: Providing feature importance through counterfactuals
- **Model Validation**: Testing model reliability through adversarial examples

## ðŸ”¬ Advanced Usage

### Custom Parameters
To modify the script for different configurations:
- Change `method='random'` to other DiCE methods (genetic, kdtree)
- Adjust `total_cfs=2` for more counterfactual candidates
- Modify model parameters in RandomForestClassifier
- Add different distance metrics (L1, Mahalanobis)

### Different Datasets
To use with other datasets:
- Change `"data/Spambase.csv"` to your dataset path
- Ensure dataset is compatible with the robustness framework
- Adjust continuous/categorical feature specifications for DiCE

## ðŸ“ž Support

If you encounter issues:
1. Run `python check_dice_installation.py` to verify setup
2. Check the generated log file for detailed error messages
3. Ensure all dependencies are correctly installed
4. Verify the Spambase.csv dataset is in the data/ directory

## ðŸŽ“ Citation

This implementation is based on:
- DiCE ML framework for counterfactual generation
- Robustness testing framework for tree-based models
- Standard machine learning quality metrics for counterfactual evaluation 